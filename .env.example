################################################################################
# Crawl4AI Two-Phase Hybrid Crawler v5.0.0
# PRODUCTION-READY ENVIRONMENT CONFIGURATION
# 
# Official Documentation: https://docs.crawl4ai.com
# GitHub: https://github.com/unclecode/crawl4ai
#
# ⚠️ CRITICAL SECURITY: Add .env to .gitignore BEFORE FIRST COMMIT!
#    echo ".env" >> .gitignore
#    echo ".env.local" >> .gitignore
#    git add .gitignore && git commit -m "Add .env to gitignore"
#
# Version: 5.0.0 (PRODUCTION READY - All Critical Issues Fixed)
# Last Updated: January 2025
# Architecture: Two-Phase Hybrid (BFS Systematic Exploration + Semantic Validation)
################################################################################

################################################################################
# SECTION 1: REQUIRED API KEYS (Application will NOT start without these)
################################################################################

# ============================================================================
# DeepSeek API Key - REQUIRED FOR ANSWER GENERATION
# ============================================================================
# 
# Status: ✅ REQUIRED - Application will NOT start without this
#
# Purpose: Answer generation using deepseek-reasoner model
#          Provides comprehensive, reasoning-based responses to queries
#
# Get your key from: https://platform.deepseek.com/api_keys
#
# Instructions:
#   1. Go to https://platform.deepseek.com/
#   2. Sign in or create account
#   3. Navigate to "API Keys" section
#   4. Click "Create API Key"
#   5. Copy the key (format: sk-xxxxx...)
#   6. Paste below after the = sign
#
# Format: sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx (typically 32+ characters)
#
# Verification: Application logs will show:
#   "DEEPSEEK_API_KEY configured successfully"
#
# Example (FAKE - DO NOT USE):
#   DEEPSEEK_API_KEY=sk-1234567890abcdefghijklmnopqrstuv
#
# If missing, you'll see error:
#   HTTPException(500): "Server misconfiguration: DEEPSEEK_API_KEY not set"
#
DEEPSEEK_API_KEY=your_deepseek_api_key_here


# ============================================================================
# OpenRouter API Key - OPTIONAL (Recommended for better re-ranking)
# ============================================================================
#
# Status: ⭐ OPTIONAL BUT HIGHLY RECOMMENDED
#
# Purpose: Phase 2b - Semantic re-ranking of crawled pages
#          Uses qwen/qwen3-embedding-8b for high-quality embeddings
#          Improves answer quality by 30-40% (typical improvement)
#
# Get your key from: https://openrouter.ai/keys
#
# Instructions:
#   1. Go to https://openrouter.ai/
#   2. Sign in or create account
#   3. Click "Keys" in left sidebar
#   4. Click "Create new key"
#   5. Copy the key (format: sk-or-xxxxx...)
#   6. Paste below after the = sign
#
# Format: sk-or-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx (typically 35+ characters)
#
# Without OpenRouter:
#   ✓ Application still works normally
#   ✓ Uses local semantic scores (good baseline)
#   ✗ Skips Phase 2b re-ranking (lower quality)
#   ✗ May miss highly relevant pages
#
# With OpenRouter:
#   ✓ Phase 2b re-ranking enabled (RECOMMENDED)
#   ✓ 75% weight on semantic similarity (proven better)
#   ✓ 25% weight on original BFS scores
#   ✓ Better handling of legal/technical documents
#   ✓ Typical improvement: 30-40% better answers
#
# Cost estimate:
#   - Typical crawl: 10-15 pages re-ranked = ~$0.0002
#   - Daily cost (10 crawls): ~$0.002
#   - Monthly cost (100+ crawls): $0.50-2.00
#
# Example (FAKE - DO NOT USE):
#   OPENROUTER_API_KEY=sk-or-1234567890abcdefghijklmnopqrstuv
#
OPENROUTER_API_KEY=your_openrouter_api_key_here


################################################################################
# SECTION 2: SERVER CONFIGURATION
################################################################################

# ============================================================================
# Server Port Configuration
# ============================================================================
#
# Default: 8080
#
# Usage:
#   - Local development: Set to 8080 (or any available port)
#   - Docker container: Usually 8080 (mapped from container)
#   - Cloud platforms (Zeabur, Vercel, etc): LEAVE UNSET
#     (Platform auto-configures the PORT variable)
#
# Zeabur Instructions:
#   - Do NOT set PORT in .env
#   - Zeabur will automatically inject PORT at runtime
#   - If you set it, Zeabur will override your value
#
# Port availability:
#   - Linux: lsof -i :8080 (check if port is in use)
#   - macOS: lsof -i :8080
#   - Windows: netstat -ano | findstr :8080
#
# If port is in use, choose alternative:
#   - PORT=8081  (typically available)
#   - PORT=3000  (common for dev)
#   - PORT=5000  (another common choice)
#
PORT=8080


# ============================================================================
# CORS (Cross-Origin Resource Sharing) Configuration
# ============================================================================
#
# Default: * (Allow all origins - suitable for development)
#
# CORS_ORIGINS controls which domains can call your API
#
# Examples:
#
#   Allow all (development):
#   CORS_ORIGINS=*
#
#   Allow specific domains (production recommended):
#   CORS_ORIGINS=https://example.com,https://app.example.com
#
#   Allow localhost for testing:
#   CORS_ORIGINS=http://localhost:3000,http://localhost:8000
#
#   Allow any subdomain:
#   CORS_ORIGINS=https://*.example.com
#
# Security Note:
#   - Production: Restrict to known domains
#   - Development: Can use * for convenience
#   - Always use HTTPS in production (except localhost)
#
# Multiple origins (comma-separated, NO spaces):
#   CORS_ORIGINS=https://example.com,https://app.example.com,http://localhost:3000
#
CORS_ORIGINS=*


################################################################################
# SECTION 3: API AUTHENTICATION & SECURITY
################################################################################

# ============================================================================
# API Key Authentication - Enable/Disable
# ============================================================================
#
# Default: false (Authentication disabled)
#
# Purpose: Protect your API from unauthorized access
#
# REQUIRE_API_KEY=false (recommended for development/internal use):
#   ✓ No authentication required
#   ✓ Simpler testing
#   ✗ Not secure for public deployment
#
# REQUIRE_API_KEY=true (recommended for production):
#   ✓ Clients must provide Bearer token
#   ✓ Protects against abuse
#   ✓ Track usage per API key
#   ✗ Requires coordinating keys with clients
#
# How it works when enabled:
#   1. Client sends: curl -H "Authorization: Bearer YOUR_CRAWL_API_KEY" ...
#   2. Server validates the key
#   3. If valid: Request proceeds
#   4. If invalid: Returns HTTP 403 Forbidden
#
# To enable authentication:
#   1. Set REQUIRE_API_KEY=true below
#   2. Set CRAWL_API_KEY to a strong key (32+ characters) below
#   3. Share CRAWL_API_KEY securely with API clients
#   4. Clients include in all requests: -H "Authorization: Bearer $CRAWL_API_KEY"
#
# Generating a strong API key:
#   - Linux/macOS: python3 -c "import secrets; print(secrets.token_urlsafe(32))"
#   - Online: https://www.random.org/strings/ (alphanumeric, 32+ chars)
#   - Or: uuid + hash (example below)
#
REQUIRE_API_KEY=false


# ============================================================================
# Application API Key (required if REQUIRE_API_KEY=true)
# ============================================================================
#
# Status: Required only if REQUIRE_API_KEY=true
#
# Purpose: Clients include this in Authorization header
#          Format: Authorization: Bearer YOUR_CRAWL_API_KEY
#
# Generate a strong key:
#   - At least 32 characters
#   - Use alphanumeric + underscore/hyphen
#   - Keep secret (never commit to git)
#   - Rotate periodically (quarterly minimum)
#
# Example strong key (FAKE - DO NOT USE):
#   CRAWL_API_KEY=sk_prod_1234567890abcdefghijklmnopqrstuv
#
# Usage example:
#   curl -X POST http://localhost:8080/crawl \
#     -H "Content-Type: application/json" \
#     -H "Authorization: Bearer sk_prod_1234567890abcdefghijklmnopqrstuv" \
#     -d '{"start_url": "https://example.com", "query": "test"}'
#
# If REQUIRE_API_KEY=false, this variable is ignored
#
CRAWL_API_KEY=your_application_api_key_here


################################################################################
# SECTION 4: LOGGING & MONITORING
################################################################################

# ============================================================================
# Log File Location
# ============================================================================
#
# Default: /tmp/crawl4ai.log
#
# Purpose: Structured JSON logging for production debugging
#          All requests, crawls, and errors are logged here
#
# Log Format:
#   - JSON format (machine-readable)
#   - Timestamp, log level, message, request ID
#   - Exception traceback on errors
#   - Performance metrics (timing, memory usage)
#
# Examples:
#
#   Linux/macOS (temporary, cleared on reboot):
#   LOG_FILE=/tmp/crawl4ai.log
#
#   Linux/macOS (persistent):
#   LOG_FILE=/var/log/crawl4ai/crawl4ai.log
#
#   Docker (inside container):
#   LOG_FILE=/app/logs/crawl4ai.log
#   (Mount volume: -v /host/logs:/app/logs)
#
#   Development (current directory):
#   LOG_FILE=./crawl4ai.log
#
# Permissions:
#   - Directory must be writable by application
#   - Docker: Usually /app/ is writable
#   - Linux: /var/log/ requires elevated permissions
#   - Windows: Use absolute path like C:/logs/crawl4ai.log
#
# Monitoring the log:
#   tail -f /tmp/crawl4ai.log  # Real-time tail
#   grep ERROR /tmp/crawl4ai.log  # Find errors
#   grep request_id=abc123 /tmp/crawl4ai.log  # Specific request
#
LOG_FILE=/tmp/crawl4ai.log


# ============================================================================
# Memory Threshold for Graceful Degradation
# ============================================================================
#
# Default: 2000 (MB) - UPDATED v5.0.1
#
# Purpose: Monitor memory usage and stop crawling if threshold exceeded
#          Prevents out-of-memory crashes in production
#
# IMPORTANT: The application loads ~1.5-2GB at startup:
#   - Sentence-transformers model: ~500MB
#   - PyTorch: ~800MB
#   - Chromium browser: ~200MB
#   - Application + dependencies: ~200MB
#   Therefore, threshold MUST be higher than startup memory!
#
# How it works:
#   1. Before each page crawl, check current process memory (RSS)
#   2. If memory > threshold: Stop crawling (save memory)
#   3. Generate answer from pages already crawled (graceful degradation)
#   4. Log warning in structured logs
#
# Typical memory usage:
#   - Initial: ~1500-2000 MB (application startup with models)
#   - Per page: ~5-20 MB (depends on content size)
#   - With 50 pages: ~2000-2500 MB total
#
# Recommendations:
#
#   Zeabur/Cloud (2GB RAM plan):
#   MEMORY_THRESHOLD_MB=1800  # Tight, may limit crawl depth
#
#   Zeabur/Cloud (4GB RAM plan - RECOMMENDED):
#   MEMORY_THRESHOLD_MB=3500  # Good balance
#
#   Docker (8GB RAM available):
#   MEMORY_THRESHOLD_MB=6000  # Stop at 6GB, leave 2GB buffer
#
#   Server (16GB RAM available):
#   MEMORY_THRESHOLD_MB=12000  # Stop at 12GB, leave 4GB buffer
#
# Requires psutil:
#   pip install psutil
#   (Included in requirements.txt)
#
# If psutil not installed:
#   - Memory monitoring is skipped (logged as warning)
#   - MEMORY_THRESHOLD_MB is ignored
#   - Application continues normally
#
MEMORY_THRESHOLD_MB=2000


################################################################################
# SECTION 5: OPTIONAL: HUGGING FACE CACHE CONFIGURATION
################################################################################

# ============================================================================
# Hugging Face Models Cache Directory
# ============================================================================
#
# Status: OPTIONAL (Only customize if needed)
#
# Default: /app/.cache/huggingface (in Docker)
#          ~/.cache/huggingface (on host machine)
#
# Purpose: Cache directory for sentence-transformers embeddings model
#          Model: paraphrase-multilingual-mpnet-base-v2 (~430MB)
#          Downloaded on first startup
#
# When to set HF_HOME:
#
#   1. Custom storage location (large disk)
#   2. Docker volume persistence
#   3. Shared cache across containers
#   4. Development with multiple models
#
# Examples:
#
#   Docker with volume mount:
#   HF_HOME=/models/huggingface
#   (Include in docker run: -v /host/models:/models)
#
#   Development (download models once):
#   HF_HOME=/Users/username/.cache/huggingface
#
#   Server (shared cache):
#   HF_HOME=/opt/ml/cache/huggingface
#
# Model download details:
#   - Size: ~430 MB
#   - Time: 2-5 minutes (first run only)
#   - Cached: Subsequent runs use cached model
#   - No further downloads needed
#
# Verify cached model exists:
#   ls -la $HF_HOME/models/sentences-transformers/
#
# Cache verification in logs:
#   "Embedding model initialized successfully"
#   "Embedding model verification PASSED"
#
# Leave commented to use default:
# HF_HOME=/app/.cache/huggingface


################################################################################
# SECTION 6: ADVANCED CONFIGURATION (Usually not needed)
################################################################################

# ============================================================================
# Optional: Custom Request Timeout (seconds)
# ============================================================================
# Default: 60 seconds per page crawl
# Uncomment to customize:
# REQUEST_TIMEOUT_SECONDS=60

# Optional: Custom DeepSeek Timeout (seconds)
# Default: 60 seconds for answer generation
# Uncomment to customize:
# DEEPSEEK_TIMEOUT_SECONDS=60

# Optional: Custom OpenRouter Timeout (seconds)
# Default: 30 seconds for embedding requests
# Uncomment to customize:
# OPENROUTER_TIMEOUT_SECONDS=30


################################################################################
# SECTION 7: SETUP INSTRUCTIONS & DEPLOYMENT
################################################################################

# ============================================================================
# LOCAL DEVELOPMENT SETUP
# ============================================================================
#
# 1. Secure .env first (CRITICAL):
#    echo ".env" >> .gitignore
#    echo ".env.local" >> .gitignore
#    git add .gitignore && git commit -m "Add .env to gitignore"
#
# 2. Copy this file to .env:
#    cp .env.example .env
#
# 3. Get API Keys:
#
#    a) DeepSeek (REQUIRED):
#       - Go to: https://platform.deepseek.com/api_keys
#       - Create new API key
#       - Copy entire key (sk-xxx...)
#       - Edit .env: DEEPSEEK_API_KEY=<paste_here>
#
#    b) OpenRouter (RECOMMENDED):
#       - Go to: https://openrouter.ai/keys
#       - Create new API key
#       - Copy entire key (sk-or-xxx...)
#       - Edit .env: OPENROUTER_API_KEY=<paste_here>
#
# 4. Edit .env:
#    nano .env  (or vim, code, etc)
#
# 5. Verify .env:
#    grep -E "DEEPSEEK_API_KEY|OPENROUTER_API_KEY" .env
#    (Should show both keys, not "here" placeholders)
#
# 6. Start the application:
#    python main.py
#
#    Expected output:
#    - "Starting Crawl4AI v5.0.0 on port 8080"
#    - "Embedding Model Ready: True"
#    - "BFSDeepCrawlStrategy Available: True"
#
# 7. Test the API:
#    curl http://localhost:8080/health
#    # Should return: {"status": "healthy"}
#
#    curl -X POST http://localhost:8080/crawl \
#      -H "Content-Type: application/json" \
#      -d '{
#        "start_url": "https://example.com",
#        "query": "What is this website about?",
#        "max_pages": 20
#      }'


# ============================================================================
# DOCKER DEPLOYMENT (Production Recommended)
# ============================================================================
#
# 1. Build image:
#    docker build -t crawl4ai:v5.0.0 .
#
# 2. Verify .env is ready:
#    grep -E "^DEEPSEEK_API_KEY=sk-" .env
#    (Should show your key, not placeholder)
#
# 3. Run with .env file:
#    docker run -d \
#      --name crawl4ai-prod \
#      --restart unless-stopped \
#      -p 8080:8080 \
#      --env-file .env \
#      --shm-size=2g \
#      --memory=8g \
#      --cpus=4 \
#      -v /var/log/crawl4ai:/app/logs \
#      crawl4ai:v5.0.0
#
# 4. Check logs:
#    docker logs crawl4ai-prod -f
#
# 5. Health check:
#    curl http://localhost:8080/health
#
# 6. View structured logs:
#    cat /var/log/crawl4ai/crawl4ai.log | python -m json.tool
#
# 7. Stop container:
#    docker stop crawl4ai-prod
#    docker rm crawl4ai-prod


# ============================================================================
# ZEABUR CLOUD DEPLOYMENT
# ============================================================================
#
# 1. Prepare repository:
#    - Ensure .env is in .gitignore (CRITICAL)
#    - Commit: main.py, requirements.txt, Dockerfile
#    - Push to GitHub
#
# 2. Connect to Zeabur:
#    - Go to https://zeabur.com
#    - Create account (free)
#    - Click "New Project"
#    - Select GitHub repository
#
# 3. Set environment variables in Zeabur dashboard:
#    - Go to "Settings" → "Environment Variables"
#    - Click "Add Variable"
#    - Name: DEEPSEEK_API_KEY
#    - Value: sk-xxxxxxxxxxxx (your actual key)
#    - Click "Add"
#    - Repeat for OPENROUTER_API_KEY
#    - Leave PORT unset (Zeabur auto-configures)
#
# 4. Deploy:
#    - Click "Deploy" button
#    - Wait 10-15 minutes for first build
#    - Check "Logs" tab for errors
#
# 5. Access deployed API:
#    - URL: https://your-app.zeabur.app/docs (Swagger UI)
#    - Health: https://your-app.zeabur.app/health
#    - API: https://your-app.zeabur.app/crawl
#
# 6. Test deployment:
#    curl -X POST https://your-app.zeabur.app/crawl \
#      -H "Content-Type: application/json" \
#      -d '{
#        "start_url": "https://example.com",
#        "query": "test query",
#        "max_pages": 20
#      }'


################################################################################
# SECTION 8: ARCHITECTURE OVERVIEW (v5.0.0 Two-Phase Hybrid)
################################################################################

# ============================================================================
# Phase 1: BFS Systematic Exploration
# ============================================================================
#
# Purpose: Comprehensively map the website structure
#
# How it works:
#   1. Start from seed URL
#   2. Extract all links using BFSDeepCrawlStrategy
#   3. Process pages level-by-level (breadth-first)
#   4. Rank links by relevance to query (low threshold: 0.1)
#   5. Crawl all pages up to max_depth
#   6. Result: 40-100+ pages (comprehensive coverage)
#
# Why BFS?
#   ✓ Systematic coverage of website structure
#   ✓ Finds pages at all depths (not just top-level)
#   ✓ Better for legal/documentation sites (often deep hierarchies)
#   ✓ Discovers all relevant content before Phase 2 filtering
#
# Configuration in main.py:
#   BFSDeepCrawlStrategy(
#       max_depth=3,           # 3 levels deep
#       max_pages=100,         # Up to 100 pages
#       score_threshold=0.1    # Low threshold (inclusive)
#   )
#
# Output: List of crawled pages with content and URLs


# ============================================================================
# Phase 2: Adaptive Semantic Validation
# ============================================================================
#
# Purpose: Filter to high-confidence pages relevant to query
#
# How it works:
#   1. For each crawled page, compute semantic score
#   2. Use local embedding model (runs on your server)
#   3. Score = cosine similarity between query and page content
#   4. Filter: Keep only pages above relevance_threshold
#   5. Result: 15-30 high-confidence pages
#
# Why semantic validation?
#   ✓ Removes navigation pages, footers, etc. (low relevance)
#   ✓ Identifies pages truly related to query
#   ✓ Reduces context size for Phase 3 (cost/quality)
#   ✓ Runs locally (no API calls, fast)
#
# Local embedding model:
#   - Model: sentence-transformers/paraphrase-multilingual-mpnet-base-v2
#   - Size: ~430 MB (downloaded once, cached)
#   - Languages: 50+ (English, Chinese, etc.)
#   - No API key needed (runs on your server)
#
# Configuration in main.py:
#   relevance_threshold=0.3  # Default (configurable per request)
#   (Keep pages with score >= 0.3)
#
# Output: Filtered list of relevant pages (typically 15-30)


# ============================================================================
# Phase 2b (Optional): OpenRouter Re-ranking
# ============================================================================
#
# Status: Optional but RECOMMENDED
#
# Purpose: Additional semantic refinement using high-quality embeddings
#
# How it works:
#   1. Send top pages to OpenRouter API
#   2. Get embeddings using qwen3-embedding-8b model
#   3. Recompute similarity scores (higher quality than local model)
#   4. Weight: 75% semantic + 25% original BFS score
#   5. Result: Best 15 pages (proven better quality)
#
# Why Phase 2b?
#   ✓ Higher-quality embeddings (qwen3-embedding-8b)
#   ✓ Better understanding of legal/technical documents
#   ✓ Hybrid weighting (semantic + BFS authority)
#   ✓ Minimal cost (~$0.0002 per crawl)
#
# Requires: OPENROUTER_API_KEY (optional but recommended)
#
# Without Phase 2b:
#   - Uses Phase 2 semantic scores only
#   - Still produces good answers
#   - May miss subtle relevance indicators
#
# With Phase 2b:
#   - Re-ranked with premium embeddings
#   - 30-40% quality improvement (typical)
#   - Better handles complex legal/technical queries
#
# Configuration in main.py:
#   await rerank_with_openrouter(
#       query=query,
#       pages=validated_pages,
#       embedding_client=openrouter_client,
#       top_k=15  # Top 15 pages for Phase 3
#   )
#
# Output: Best 15 pages (re-ranked)


# ============================================================================
# Phase 3: Answer Generation with DeepSeek
# ============================================================================
#
# Purpose: Generate comprehensive, reasoned answer from validated pages
#
# How it works:
#   1. Format top 15 validated pages as context
#   2. Send to DeepSeek-reasoner with system prompt
#   3. Model reasons about the content (advanced)
#   4. Generates comprehensive, accurate answer
#   5. Includes citations to source URLs
#
# Why DeepSeek-reasoner?
#   ✓ Advanced reasoning capabilities
#   ✓ Better understanding of complex documents
#   ✓ Explicit reasoning steps (transparent)
#   ✓ High accuracy for legal/technical questions
#
# Graceful degradation:
#   - If DeepSeek fails: Uses fallback answer (text extraction)
#   - Fallback: Top 3 pages + content snippets
#   - Application continues working (never fails)
#
# Requires: DEEPSEEK_API_KEY (required)
#
# Configuration in main.py:
#   await call_deepseek(
#       query=query,
#       context=formatted_pages,
#       api_key=deepseek_api_key,
#       max_tokens=2000
#   )
#
# Output: Comprehensive answer with citations


# ============================================================================
# End-to-End Example Flow
# ============================================================================
#
# User Query: "What are the penalties for copyright infringement?"
#
# Step 1 - Phase 1 (BFS Exploration):
#   - Start: https://copyright-guide.example.com/
#   - Extract 200+ links across 3 depth levels
#   - Crawl: 80 pages (BFS comprehensive)
#   - Content: Full text, metadata, URLs
#
# Step 2 - Phase 2 (Semantic Validation):
#   - Score each page for relevance to query
#   - Page "penalties.html": Score 0.92 (highly relevant)
#   - Page "footer.html": Score 0.05 (not relevant)
#   - Filter: Keep 24 pages with score >= 0.3
#
# Step 3 - Phase 2b (OpenRouter Re-ranking, if enabled):
#   - Get premium embeddings for 24 pages
#   - Recompute scores (qwen3 model)
#   - Top 15 pages: 0.89, 0.87, 0.85... (highest quality)
#
# Step 4 - Phase 3 (Answer Generation):
#   - Format top 15 pages as context
#   - Send to DeepSeek-reasoner
#   - Model reasons about penalties
#   - Response: "Copyright infringement penalties include..."
#   - Citations: URLs of source pages
#
# Result: Comprehensive, accurate, cited answer


################################################################################
# SECTION 9: TROUBLESHOOTING & SUPPORT
################################################################################

# ============================================================================
# Common Issues & Solutions
# ============================================================================
#
# ERROR: "DEEPSEEK_API_KEY environment variable not set"
#   Cause: DEEPSEEK_API_KEY not configured in .env
#   Solution:
#     1. Check file exists: cat .env | grep DEEPSEEK_API_KEY
#     2. Verify not commented: Should start with DEEPSEEK_API_KEY=
#     3. Verify has value: Should NOT be "here" placeholder
#     4. Restart application after editing .env
#
# ERROR: "No API key provided" or "API key invalid"
#   Cause: API key format incorrect or invalid
#   Solution:
#     1. Check DeepSeek key format: sk-xxxxxxxx (32+ chars)
#     2. Check for extra spaces: DEEPSEEK_API_KEY=sk-xxx (no trailing space)
#     3. Verify key is active: Login to https://platform.deepseek.com/
#     4. Try regenerating key
#
# ERROR: "Connection timeout" or "Cannot reach API"
#   Cause: Network issue or API server down
#   Solution:
#     1. Check internet connection: ping api.deepseek.com
#     2. Check firewall: Verify port 443 is open
#     3. Built-in retry logic: Auto-retries up to 3 times (40-60 seconds)
#     4. Check API status: https://status.deepseek.com/
#
# ERROR: "Memory threshold exceeded"
#   Cause: Too many pages loaded (crawler uses >500MB)
#   Solution:
#     1. Reduce max_pages in request (default 100, try 50)
#     2. Reduce max_depth in request (default 3, try 2)
#     3. Increase MEMORY_THRESHOLD_MB (if resources available)
#     4. Verify server has enough RAM (8GB recommended minimum)
#
# ERROR: "No content extracted" or "Empty answer"
#   Cause: Website structure unusual or content not found
#   Solution:
#     1. Check start URL is accessible: curl https://example.com
#     2. Check robots.txt allows crawling
#     3. Check website is not behind login
#     4. Try with simpler query
#     5. Check application logs: tail -f /tmp/crawl4ai.log
#
# ERROR: "HTTP 429 - Rate Limited"
#   Cause: DeepSeek or OpenRouter rate limit hit
#   Solution:
#     - Built-in retry: Auto-retries with exponential backoff
#     - Wait 1-2 minutes before retrying
#     - Check API quotas: https://platform.deepseek.com/account
#     - Upgrade account if needed
#
# ERROR: "Port already in use"
#   Cause: Something else is using port 8080
#   Solution:
#     1. Check what's using the port: lsof -i :8080
#     2. Kill process: kill -9 <PID>
#     3. Or use different port: PORT=8081 python main.py
#
# ERROR: "Permission denied" (Linux/Docker)
#   Cause: Log file directory not writable
#   Solution:
#     1. Check directory exists: mkdir -p /tmp/crawl4ai
#     2. Check permissions: chmod 755 /tmp/crawl4ai
#     3. Check ownership: chown application_user:group /tmp/crawl4ai
#     4. Or change LOG_FILE to writable location


# ============================================================================
# Getting Help
# ============================================================================
#
# Documentation: https://docs.crawl4ai.com
# GitHub Issues: https://github.com/unclecode/crawl4ai/issues
# Discussion Forum: https://github.com/unclecode/crawl4ai/discussions
# Email: support@crawl4ai.com
#
# When reporting issues, include:
#   1. .env configuration (without API keys!)
#   2. Full error message from logs
#   3. System info: OS, Python version, Docker version
#   4. Steps to reproduce the issue
#   5. Relevant logs: tail -100 /tmp/crawl4ai.log


################################################################################
# SECTION 10: COST ESTIMATES & PRICING (January 2025)
################################################################################

# ============================================================================
# API Pricing (Subject to change - verify at source)
# ============================================================================
#
# DeepSeek-reasoner (Answer Generation):
#   Input:  ~$0.55 per 1M tokens
#   Output: ~$2.19 per 1M tokens
#
#   Typical per-crawl cost:
#     Input tokens (context): ~1-2K
#     Output tokens (answer): ~2-4K
#     Cost per crawl: $5-15 (using reasoner)
#
#   Monthly estimates:
#     Light use (1-2 crawls/day): $20-50
#     Medium use (5-10 crawls/day): $100-200
#     Heavy use (20+ crawls/day): $300-500
#
# OpenRouter (Re-ranking - Optional):
#   Cost: ~$0.02 per 1M tokens (very cheap)
#
#   Typical per-crawl cost:
#     20 pages × 2K tokens = ~40K tokens
#     Cost: ~$0.0008 per crawl
#
#   Monthly estimates:
#     Light use: $0.25
#     Heavy use: $2-5
#
# Total Estimated Cost:
#   DeepSeek: Primary cost (most queries)
#   OpenRouter: Optional, minimal add-on (recommended)
#
# Cost Optimization:
#   1. Reduce max_pages (fewer pages = lower context)
#   2. Use relevance_threshold (filter more aggressively)
#   3. Skip OpenRouter for simple queries
#   4. Batch requests during off-peak hours
#
# Free Tier Options:
#   - DeepSeek: Free trial tokens ($5 credit)
#   - OpenRouter: Free trial available


# ============================================================================
# More detailed cost analysis examples:
# ============================================================================
#
# Scenario 1: Light Use (1-2 queries per day)
#   - Crawls: 2/day × 30 days = 60 crawls/month
#   - DeepSeek cost: 60 × $10 = $600/month (reasoner model)
#   - OpenRouter: 60 × $0.001 = $0.06/month
#   - Total: ~$600/month
#   - Note: Reasoner is powerful but expensive; consider faster models
#
# Scenario 2: Medium Use (10 queries per day)
#   - Crawls: 10/day × 30 days = 300 crawls/month
#   - Using faster DeepSeek model (5-6/crawl): ~$1800-1800/month
#   - OpenRouter: 300 × $0.001 = $0.30/month
#   - Total: ~$1800+/month
#   - Recommendation: Negotiate volume pricing with DeepSeek
#
# Scenario 3: Heavy Use (100+ queries per day)
#   - Contact DeepSeek for enterprise pricing
#   - Likely significant discount (50%+ off)
#   - Set up dedicated support channel
#   - Consider on-premise deployment
#
# Cost Tips:
#   1. Start with cheapest model, upgrade as needed
#   2. Use query parameters to limit scope
#   3. Cache results when possible
#   4. Monitor token usage carefully
#   5. Set alerts for cost overruns


################################################################################
# SECTION 11: SECURITY BEST PRACTICES
################################################################################

# ============================================================================
# API Key Security (CRITICAL)
# ============================================================================
#
# ✅ DO:
#   - Store .env in .gitignore BEFORE FIRST COMMIT (non-negotiable)
#   - Use different keys for dev/staging/production
#   - Rotate keys quarterly (minimum)
#   - Restrict key permissions at source (e.g., read-only if possible)
#   - Use environment variables in production (never hardcode)
#   - Keep backup keys (in case of compromise)
#   - Monitor API usage for suspicious activity
#   - Use strong CRAWL_API_KEY (32+ random characters)
#
# ❌ DON'T:
#   - Commit .env to GitHub (even private repos)
#   - Share API keys in chat, email, or documentation
#   - Paste keys in logs or console output
#   - Use same key across multiple environments
#   - Include keys in error messages shown to users
#   - Check keys into version control history (even accidentally)
#   - Use predictable key patterns
#   - Store in plain-text configuration files outside .env
#
# If API Key Compromised:
#   1. IMMEDIATELY revoke key in API dashboard
#   2. Check API logs for unusual activity
#   3. Generate new key
#   4. Update .env with new key
#   5. Restart application
#   6. Review access logs for unauthorized use
#
# Production Deployment:
#   - Use cloud provider's secret management (AWS Secrets, Azure Key Vault)
#   - Set environment variables in deployment platform (Zeabur, Vercel, etc)
#   - Enable API key rotation policies
#   - Use separate keys per environment (dev, staging, prod)
#   - Monitor rate limits and quotas


# ============================================================================
# Network Security
# ============================================================================
#
# Firewall Rules:
#   - Allow inbound: Port 8080 (or configured PORT)
#   - Allow outbound: Port 443 (HTTPS) to API endpoints
#   - Restrict access: Use firewall rules if possible
#   - VPN: Consider VPN for internal deployments
#
# HTTPS/TLS:
#   - Production: Always use HTTPS (not HTTP)
#   - Use reverse proxy (nginx, traefik) for TLS termination
#   - Certificate: Use Let's Encrypt (free) or commercial
#   - SSL/TLS version: Enforce TLS 1.2+ (TLS 1.3 recommended)
#
# Rate Limiting:
#   - Built-in: 3 concurrent requests (MAX_CONCURRENT_REQUESTS)
#   - Additional: Use reverse proxy rate limiting
#   - API: Check DeepSeek/OpenRouter rate limits
#   - Monitor: Track usage per client


# ============================================================================
# Access Control
# ============================================================================
#
# Enable Authentication (Recommended for Production):
#   - Set REQUIRE_API_KEY=true
#   - Set strong CRAWL_API_KEY (32+ chars)
#   - Distribute key securely to clients
#   - Rotate key periodically
#
# CORS Configuration:
#   - Restrict CORS_ORIGINS to known domains
#   - Never use * in production (unless internal-only)
#   - Use HTTPS origins only (except localhost)
#   - Add cross-site request forgery (CSRF) protection if needed


################################################################################
# EOF - .env.example v5.0.0 PRODUCTION READY
################################################################################

# Last updated: January 2025
# Version: 5.0.0
# Architecture: Two-Phase Hybrid Crawler (BFS + Semantic Validation)
# Status: PRODUCTION READY - All critical issues fixed
#
# For updates: https://docs.crawl4ai.com
# For issues: https://github.com/unclecode/crawl4ai/issues
