# Crawl4AI Adaptive Crawler v3.7.0 Environment Variables
# This is a TEMPLATE file - do NOT put real API keys here!
# Copy this to .env and fill in your actual keys
#
# ⚠️ IMPORTANT: Add .env to .gitignore!
#    echo ".env" >> .gitignore
#    echo ".env.local" >> .gitignore

# ============================================================
# REQUIRED: DeepSeek API Key for answer generation
# ============================================================
# ⚠️ REQUIRED - Application will NOT start without this!
#
# Get your API key from: https://platform.deepseek.com/
# Used for: Generating comprehensive answers from crawled content
# Model: deepseek-reasoner (advanced reasoning)
#
# Format: sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx (typically 32+ chars)
#
# If missing, you'll see error:
#   "DEEPSEEK_API_KEY environment variable not set"
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# ============================================================
# OPTIONAL: OpenRouter API Key for re-ranking phase
# ============================================================
# Get your API key from: https://openrouter.ai/
# Model used: qwen/qwen3-embedding-8b (high-quality embeddings)
#
# Format: sk-or-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx (typically 35+ chars)
#
# RE-RANKING PHASE (optional but RECOMMENDED):
#   - After crawling, re-ranks pages by semantic similarity
#   - Uses OpenRouter embeddings for high-quality scoring
#   - Helps surface the most relevant content (SOLUTION 3)
#   - Weighting: 75% semantic + 25% BM25 (SOLUTION 3 improvement)
#
# Without OpenRouter:
#   - Uses crawl scores only (still works, less accurate)
#   - Skips re-ranking phase automatically
#   - Application continues to work normally
OPENROUTER_API_KEY=your_openrouter_api_key_here

# ============================================================
# OPTIONAL: Server port configuration
# ============================================================
# Local development: Set to 8080 (default)
# Zeabur deployment: Usually auto-configured, do NOT override
# 
# Default: 8080
# On Zeabur: Leave unset (Zeabur sets automatically)
# 
# Set only if:
#   - Running locally on different port
#   - Deploying to custom server
PORT=8080

# ============================================================
# OPTIONAL: Hugging Face cache directory
# ============================================================
# Default: /app/.cache/huggingface (inside Docker)
# Only needed if customizing cache location
# 
# Leave commented unless you have specific requirements
# HF_HOME=/app/.cache/huggingface

# ============================================================
# v3.7.0 NEW FIXES (SPA/JavaScript Support)
# ============================================================
#
# FIX 1: SPAFriendlyCrawler Wrapper
#   - ROOT CAUSE: AdaptiveCrawler doesn't accept CrawlerRunConfig
#   - SOLUTION: Wrapper intercepts arun() and injects config automatically
#   - wait_for="css:body" - Wait for JavaScript to render
#   - process_iframes=True - Include iframe content
#   - delay_before_return_html=2.0s - Extra time for JS execution
#
# FIX 2: Embedding Model Verification at Startup
#   - Tests model loading with English and Chinese text
#   - Catches silent failures early
#   - Shows embedding dimensions and norms in logs
#
# FIX 3: Verbose Logging for Debugging
#   - Logs crawl result details
#   - Shows knowledge base contents
#   - Helps trace content extraction issues
#
# ============================================================
# PREVIOUS SOLUTIONS (Still Applied)
# ============================================================
#
# SOLUTION 1: Aggressive Stopping Prevention
#   - confidence_threshold=0.05 (5% instead of 70%)
#   - min_relative_improvement=0.01 (1% instead of 10%)
#   - Prevents early termination before finding relevant pages
#   - Result: Crawls 40-50+ pages instead of stopping at ~13
#
# SOLUTION 2: Better Embedding Model for Legal Documents
#   - UPDATED: paraphrase-multilingual-mpnet-base-v2
#   - Previous: paraphrase-multilingual-MiniLM-L12-v2 (MiniLM)
#   - Better semantic understanding of complex legal terminology
#   - More accurate for Chinese legal documents
#   - Supports 50+ languages including Chinese, English, etc.
#
# SOLUTION 3: Better Re-ranking Weights
#   - UPDATED: 75% semantic + 25% BM25 (was 60/40)
#   - Trusts embedding model more than keyword matching
#   - Better for legal domain where surface keywords ≠ relevance
#
# SOLUTION 4: More Query Variations
#   - UPDATED: n_query_variations=20 (was 5)
#   - Better coverage of synonyms and related terms
#   - Especially important for Chinese queries
#
# SOLUTION 5: Deeper Crawling
#   - UPDATED: max_pages=50 (was 20)
#   - UPDATED: top_k_links=25 (was 15)
#   - Explores more of the website structure
#   - Discovers pages like lei17_cn.asp that were previously skipped
#
# SOLUTION 6: Proper Content Extraction
#   - Ensures .asp files are properly parsed
#   - Fixed markdown generation for legal documents
#   - Better term extraction for saturation detection
#
# ============================================================
# EMBEDDING STRATEGY - Runs LOCALLY (NO API NEEDED!)
# ============================================================
#
# LOCAL MULTILINGUAL MODEL:
#   - sentence-transformers/paraphrase-multilingual-mpnet-base-v2 (SOLUTION 2)
#   - Runs entirely on your server (downloaded in Docker)
#   - No API calls for embeddings during crawling
#   - Supports 50+ languages including:
#     * Chinese (Simplified & Traditional)
#     * English
#     * Portuguese
#     * And 45+ more
#
# CRAWLING PROCESS:
#   1. Start URL → Extract links
#   2. Rank links by semantic similarity (LOCAL embeddings)
#   3. Visit highest-scoring links
#   4. Extract content from each page
#   5. Continue until max_pages reached (SOLUTION 1: aggressive)
#
# RE-RANKING PHASE (Optional - requires OPENROUTER_API_KEY):
#   1. Use OpenRouter to get embeddings for all crawled pages
#   2. Re-rank by semantic similarity to query (SOLUTION 3: 75% weight)
#   3. Select top pages for answer generation
#
# ANSWER GENERATION:
#   1. Format top pages into context
#   2. Send to DeepSeek-reasoner with system prompt
#   3. Generate comprehensive answer with citations
#
# ============================================================
# SETUP INSTRUCTIONS
# ============================================================
#
# 1. Secure .env file first:
#    echo ".env" >> .gitignore
#    echo ".env.local" >> .gitignore
#    git add .gitignore && git commit -m "Add .env to gitignore"
#
# 2. Copy this file to .env:
#    cp .env.example .env
#
# 3. Get API Keys:
#    a) DeepSeek (REQUIRED):
#       - Go to: https://platform.deepseek.com/
#       - Click "API Keys" or "Console"
#       - Create new API key
#       - Copy and paste into DEEPSEEK_API_KEY
#    
#    b) OpenRouter (OPTIONAL but RECOMMENDED):
#       - Go to: https://openrouter.ai/
#       - Click "Keys" in sidebar
#       - Create new API key
#       - Copy and paste into OPENROUTER_API_KEY
#
# 4. Edit .env file:
#    # On macOS/Linux:
#    nano .env
#    
#    # On Windows:
#    notepad .env
#
# 5. Fill in your actual keys (DO NOT commit this file):
#    DEEPSEEK_API_KEY=sk-xxxxxxxxxxxx
#    OPENROUTER_API_KEY=sk-or-xxxxxxxxxxxx
#    PORT=8080
#
# 6. Start the application:
#    # Local:
#    python main.py
#    
#    # Docker:
#    docker run --env-file .env -p 8080:8080 crawl4ai-fixed:v3.7.0
#
# ============================================================
# PRICING ESTIMATES (Current as of Jan 2025)
# ============================================================
#
# ⚠️ Prices subject to change - Check APIs for current pricing
#
# DeepSeek-reasoner:
#   - Input: ~$0.55 per 1M tokens
#   - Output: ~$2.19 per 1M tokens
#   - Typical query: 1-2K tokens input, 2-4K tokens output
#   - Estimate: $5-10/month for light use
#   - Estimate: $30-50/month for heavy use (50+ crawls/day)
#
# OpenRouter (qwen3-embedding-8b):
#   - ~$0.02 per 1M tokens
#   - Typical crawl: 10-15 pages re-ranked = ~10K tokens
#   - Estimate: $0.20-0.50/month for light use
#   - Estimate: $2-5/month for heavy use
#
# Total Monthly Cost:
#   - Casual user (1-2 crawls/day): $5-10/month
#   - Power user (10+ crawls/day): $35-60/month
#
# ============================================================
# API KEY FORMATS (DO NOT USE THESE EXAMPLES!)
# ============================================================
#
# DeepSeek API Key format:
#   sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#   Typical length: 32+ characters
#   Example (FAKE): sk-1234567890abcdefghijklmnopqrstuv
#
# OpenRouter API Key format:
#   sk-or-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#   Typical length: 35+ characters
#   Example (FAKE): sk-or-1234567890abcdefghijklmnopqrstuv
#
# ⚠️ NEVER commit real keys to GitHub!
# ⚠️ NEVER share keys in chat, email, or documentation!
#
# ============================================================
# TROUBLESHOOTING
# ============================================================
#
# ERROR: "DEEPSEEK_API_KEY environment variable not set"
#   → Make sure .env file exists in project root
#   → Verify DEEPSEEK_API_KEY is set (not commented out)
#   → Check: cat .env | grep DEEPSEEK_API_KEY
#   → Restart application after editing .env
#
# ERROR: "No pages crawled" or crawler stops early
#   → SOLUTION 1 ensures aggressive crawling now
#   → Old behavior: stopped at ~13 pages
#   → New behavior: crawls 40-50+ pages
#   → If still stopping early, check:
#      - Start URL is valid and accessible
#      - Query is specific enough
#      - Website allows crawling (robots.txt)
#
# ERROR: "Lei documents not found"
#   → SOLUTION 5 (deeper crawling) now discovers these pages
#   → Old: max_pages=20, top_k_links=15
#   → New: max_pages=50, top_k_links=25
#   → Try increasing max_pages in request body
#
# ERROR: "Low semantic scores on legal documents"
#   → SOLUTION 2 (mpnet model) handles legal terminology better
#   → Old model: MiniLM (lightweight, less accurate)
#   → New model: mpnet-base-v2 (better understanding)
#   → Enable OpenRouter re-ranking for best results
#
# ERROR: "OpenRouter rate limited"
#   → Check your OpenRouter quota/credits
#   → Built-in retry logic with exponential backoff
#   → Automatic retry after 2-4 seconds
#
# ERROR: "DeepSeek API timeout"
#   → Check internet connection
#   → Built-in retry logic (up to 3 attempts)
#   → DeepSeek reasoning models can be slow (normal)
#
# ============================================================
# DOCKER SETUP
# ============================================================
#
# Build image:
#   docker build -t crawl4ai-fixed:v3.7.0 .
#
# Run with environment variables:
#   docker run -p 8080:8080 \
#     -e DEEPSEEK_API_KEY="your_key" \
#     -e OPENROUTER_API_KEY="your_key" \
#     crawl4ai-fixed:v3.7.0
#
# Or use .env file (RECOMMENDED):
#   docker run --env-file .env -p 8080:8080 crawl4ai-fixed:v3.7.0
#
# Check if service is running:
#   curl http://localhost:8080/health
#   # Should return: {"status": "healthy"}
#
# View logs:
#   docker logs <container_id> -f
#
# Get container ID:
#   docker ps | grep crawl4ai
#
# Stop container:
#   docker stop <container_id>
#
# ============================================================
# ZEABUR DEPLOYMENT
# ============================================================
#
# 1. Prepare repository:
#   - Ensure .env is in .gitignore
#   - Commit main.py, requirements.txt, Dockerfile
#   - Push to GitHub
#
# 2. Connect to Zeabur:
#   - Go to https://zeabur.com
#   - Click "New Project"
#   - Select your GitHub repository
#
# 3. Set environment variables in Zeabur console:
#   - Go to "Settings" → "Environment Variables"
#   - Add DEEPSEEK_API_KEY (REQUIRED)
#   - Add OPENROUTER_API_KEY (OPTIONAL)
#   - Leave PORT unset (Zeabur auto-configures)
#
# 4. Deploy:
#   - Click "Deploy"
#   - Wait for build to complete (~15 minutes first time)
#   - Check "Logs" tab for any errors
#
# 5. Access deployed app:
#   - URL: https://your-app.zeabur.app/docs (Swagger UI)
#   - Health check: https://your-app.zeabur.app/health
#   - API endpoint: https://your-app.zeabur.app/crawl
#
# 6. Test the deployment:
#   curl -X POST https://your-app.zeabur.app/crawl \
#     -H "Content-Type: application/json" \
#     -d '{
#       "start_url": "https://bo.dsaj.gov.mo/bo/i/95/46/codpencn/default.asp",
#       "query": "我是美国人，我在澳门吸毒被抓了会被判刑嘛？",
#       "max_pages": 50,
#       "use_embeddings": true
#     }'
#
# ============================================================
# SECURITY BEST PRACTICES
# ============================================================
#
# ✅ DO:
#   - Keep .env file in .gitignore (CRITICAL)
#   - Rotate API keys regularly
#   - Use different keys for dev/staging/production
#   - Monitor API usage and costs
#   - Keep .env files locally only
#
# ❌ DON'T:
#   - Commit .env to GitHub
#   - Share API keys in chat/email
#   - Use same keys across environments
#   - Paste API keys in logs/console output
#   - Store keys in documentation
#
# ============================================================
